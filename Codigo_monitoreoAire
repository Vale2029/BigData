//Instalamos mediante PIP la librería de Python Kafka

    pip install kafka-python

//Descargar, descomprimir y mueva de carpeta Apache Kafka

    wget https://downloads.apache.org/kafka/3.7.2/kafka_2.12-3.7.2.tgz

    tar -xzf kafka_2.13-3.7.2.tgz

    sudo mv kafka_2.12-3.7.2 /opt/kafka

//Iniciamos el servidor ZooKeeper:

    sudo /opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties &

//Iniciamos el servidor Kafka:

    sudo /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &

//Creamos un tema (topic) de Kafka, el tema se llamará calidad_aire

    /opt/kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic calidad_aire

//Crear el productor 

    nano air_quality_producer.py

//Configurar el productor con las librerias, el esquema que contiene los datos que va a generar y el tiempo que va a tomar para imprimir cada grupo de datos que es de dos segundos.

    from kafka import KafkaProducer
    import json
    import time
    import random
    
    producer = KafkaProducer(
        bootstrap_servers='localhost:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    def generate_air_quality_data():
        return {
            "pm2_5": round(random.uniform(5.0, 100.0), 2),
            "pm10": round(random.uniform(10.0, 200.0), 2),
            "co2": round(random.uniform(300.0, 5000.0), 2),
            "temperature": round(random.uniform(15.0, 40.0), 2),
            "timestamp": time.time()
        }
    
    print("Iniciando la simulación de calidad del aire...")
    while True:
        data = generate_air_quality_data()
        print(f"Enviando: {data}")
        producer.send('calidad_aire', value=data)
        time.sleep(2)  # Enviar datos cada 2 segundos


//Crear el consumidor 

    nano spark_air_quality_consumer.py

//Configurar el codigo que va a recibir los lotes de datos y analizarlos 


    from pyspark.sql import SparkSession
    from pyspark.sql.functions import from_json, col
    from pyspark.sql.types import StructType, StructField, DoubleType, StringType
    
    # Configura el nivel de log a WARN para reducir los mensajes INFO
    spark = SparkSession.builder \
        .appName("CalidadAireConsumer") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/spark_checkpoint_air") \
        .getOrCreate()
    
    # Definir el esquema de los datos de entrada
    schema = StructType([
        StructField("pm2_5", DoubleType(), True),
        StructField("pm10", DoubleType(), True),
        StructField("co2", DoubleType(), True),
        StructField("temperature", DoubleType(), True),
        StructField("timestamp", StringType(), True)
    ])
    
    # Leer el stream desde Kafka
    df = spark.readStream.format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "calidad_aire") \
        .option("startingOffsets", "latest") \
        .load()
    
    # Convertir los valores de Kafka de binario a string
    df_values = df.selectExpr("CAST(value AS STRING)")
    
    # Parsear el JSON con el esquema
    df_parsed = df_values.select(from_json(col("value"), schema).alias("data")).select("data.*")
    
    # Mostrar el resultado en consola
    query = df_parsed.writeStream \
        .outputMode("append") \
        .format("console") \
        .start()
    
    query.awaitTermination()
    
    
//EJECUCIÓN Y ANALISIS 

//En una pestaña se ejcuta el productor 

    python3 air_quality_producer.py


//En otra pestaña se ejecuta el consumidor

    spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5 spark_air_quality_consumer.py

//Para ver los datos en la web se ejecuta el siguiente enlace que contiene el IP de mi maquina virtual 

    http://192.168.1.20:4040 
